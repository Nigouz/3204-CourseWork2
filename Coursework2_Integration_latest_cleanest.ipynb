{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KZhD1QKdgcK9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import seaborn as sb\n",
    "import flask\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cu1B4nd9gmq8",
    "outputId": "42ea020f-2338-4d4c-9f1b-6916a434622f"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "# full_df = pd.read_csv(r\"C:\\Users\\Nicz\\Documents\\GitHub\\3204-CourseWork2\\shuffled-noIPV6.csv\")\n",
    "full_df = pd.read_csv(r\"shuffled-noIPV6.csv\")\n",
    "\n",
    "print(f\"[*] Shape of dataset: {full_df.shape}\")\n",
    "from sklearn.utils import shuffle\n",
    "full_df.drop(full_df.columns[0], axis=1, inplace=True)\n",
    "full_df = shuffle(full_df)\n",
    "\n",
    "print(full_df.tail())\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOgcXWdIzNSq"
   },
   "outputs": [],
   "source": [
    "full_df[\"src_port\"]=  full_df[\"src_port\"].replace(regex=\",\", value= \"\")\n",
    "full_df[\"src_port\"]=  full_df[\"src_port\"].replace(regex=\",\", value= \"\")\n",
    "full_df[\"dst_port\"]=  full_df[\"dst_port\"].replace(regex=\",\", value= \"\")\n",
    "full_df[\"dst_port\"]=  full_df[\"dst_port\"].replace(regex=\" \", value= \"\")\n",
    "full_df[\"src_port\"] = full_df[\"src_port\"].replace(regex=\" \", value=\"\")\n",
    "full_df[\"src_port\"] = full_df[\"src_port\"].replace(regex=\"dns\", value=\"53\")\n",
    "full_df[\"src_port\"] = full_df[\"src_port\"].replace(regex=\"tls\", value=\"0\")\n",
    "full_df[\"dst_port\"] = full_df[\"dst_port\"].replace(regex=\"dns\", value=\"53\")\n",
    "full_df[\"dst_ip\"] = full_df[\"dst_ip\"].replace(regex=\"\\S*:+\\S+\", value=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrekodUTmBym"
   },
   "outputs": [],
   "source": [
    "scores = {}\n",
    "scores_list= []\n",
    "\n",
    "k_value=[]\n",
    "model_scores={}\n",
    "accuracy_dict={}\n",
    "precision_dict={}\n",
    "cm_dict={}\n",
    "recall_dict={}\n",
    "f1_dict={}\n",
    "\n",
    "algo_accuracy={}\n",
    "algo_precision={}\n",
    "algo_recall={}\n",
    "algo_f1={}\n",
    "\n",
    "def scoring_metrics(y_test, y_pred, model):\n",
    "    print(f\"y_test size:{y_test.size} y_pred size:{y_pred.size}\")\n",
    "    KNN_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    KNN_precision = metrics.precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    KNN_recall = metrics.recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    KNN_f1_score = metrics.f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    \n",
    "    scores[model] = KNN_accuracy\n",
    "    scores_list.append(KNN_accuracy)\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred, labels=['-','nmap_scan', 'port_scan', 'smtp_enumeration', 'sql_enumeration', 'web_enumeration'])\n",
    "    \n",
    "    k_value.append(model)\n",
    "    accuracy_dict[model]= KNN_accuracy\n",
    "    precision_dict[model]= KNN_precision\n",
    "    recall_dict[model]= KNN_recall\n",
    "    f1_dict[model]= KNN_f1_score\n",
    "        \n",
    "    print(f\"Confusion Matrix: {cm}\")\n",
    "    \n",
    "    print(f\"\\n[*] Model: {model}\")\n",
    "    print(\"[*]Precision: {:.3f}%\".format(KNN_precision))\n",
    "    print(\"[*] Recall: {:.3f}%\".format(KNN_recall))\n",
    "\n",
    "    print(\"[*] Accuracy: {:.3f}%\".format(KNN_accuracy))\n",
    "    print(\"[*] F1_score: {:.3f}%\".format(KNN_f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7LbyI4qMQDu"
   },
   "outputs": [],
   "source": [
    "# df = full_df.head(25000)\n",
    "df = full_df.head(1000)\n",
    "data_orig = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qd8IISrRwcsl",
    "outputId": "8006e4d9-070e-4cec-9d2d-65b0caeb6d4f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Doing\")\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "df.replace(to_replace=[\"None\"], value=np.nan, inplace=True)\n",
    "clean_df = df.fillna(str(0))\n",
    "clean_x = clean_df.iloc[:, :13].values\n",
    "clean_y = clean_df[\"category\"].values\n",
    "features = df.columns.values[:-1]\n",
    "\n",
    "for label in clean_df.columns:\n",
    "    for index, rows in clean_df.iterrows():\n",
    "        new_ip = \"\"\n",
    "        ip = str(rows[label])\n",
    "        if re.search(\"\\d+\\.\\d+\\.\\d+\\.\\d+\", ip):\n",
    "            octets = ip.split(\".\")\n",
    "            for octet in octets:\n",
    "                octet = octet.rjust(3,\"0\")\n",
    "                new_ip += octet\n",
    "            clean_df[label][index] = new_ip\n",
    "\n",
    "clean_df[\"http_response_code\"] = clean_df[\"http_response_code\"].replace('HTTP/1.1\"', value=\"0\")\n",
    "clean_df[\"src_ip\"] = clean_df[\"src_ip\"].replace('::1', value=\"0\")\n",
    "clean_df[\"dst_ip\"] = clean_df[\"dst_ip\"].replace('::1', value=\"0\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HO2a6ef4Cxl3"
   },
   "outputs": [],
   "source": [
    "clean_x = clean_df.iloc[:, :13]\n",
    "column_trans = make_column_transformer((OneHotEncoder(sparse=False), ['Protocol', 'http_request_method', 'http_request_referrer', 'url_path', 'user_agent_original', 'sql_method', 'sql_query']),remainder='passthrough')\n",
    "test = column_trans.fit_transform(clean_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiRQu2ekMSlv"
   },
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation using MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "mms.fit(test)\n",
    "data_transformed = mms.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply PCA to our dataset with n_components=0.95. \n",
    "#This will select the number of components while preserving 95% of the variability in the data\n",
    "pca = PCA(n_components = 0.95)\n",
    "reduced = pca.fit_transform(data_transformed)\n",
    "label = kmeans.fit_predict(reduced)\n",
    "center = np.array(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot clustering graph with PCA\n",
    "plt.figure(figsize=(8,8))\n",
    "uniq = np.unique(label)\n",
    "for i in uniq:\n",
    "  plt.scatter(reduced[label == i , 0] , reduced[label == i , 1] , label = i)\n",
    "plt.scatter(center[:,0], center[:,1], marker=\"*\", c='black', s=250)\n",
    "plt.legend()\n",
    "plt.savefig('static/kmeans_scatterplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiRQu2ekMSlv"
   },
   "source": [
    "# Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table of Relationship between cluster and category\n",
    "data_orig['label_'] = kmeans.labels_\n",
    "ct = pd.crosstab(data_orig['category'], data_orig['label_'])\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "# print(sklearn.show_versions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiRQu2ekMSlv"
   },
   "source": [
    "# Elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the sum of squared distance between each point and the centroid in a cluster\n",
    "wcss = []\n",
    "for i in range(1,15):\n",
    "   model = KMeans(n_clusters = i)\n",
    "   model.fit(data_transformed)\n",
    "   wcss.append(model.inertia_)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(1,15), wcss)\n",
    "kn = KneeLocator(range(1,15), wcss, curve='convex', direction='decreasing')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.vlines(kn.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiRQu2ekMSlv"
   },
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZM3UFeBwmLhY"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(test, clean_y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0mYlTn8MQDw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = {}\n",
    "scores_list= []\n",
    "ktrainingtime={}\n",
    "kpredictiontime={}\n",
    "\n",
    "range_k = range(100, 160, 11)\n",
    "knn = \"\"\n",
    "skip = False\n",
    "\n",
    "for k in range_k:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    start_time = time.time()\n",
    "    knn.fit(x_train, y_train)\n",
    "    timetaken = time.time() - start_time\n",
    "    ktrainingtime[k] = timetaken\n",
    "    \n",
    "    timetaken = 0 \n",
    "    \n",
    "    start_time = time.time()\n",
    "    y_pred = knn.predict(x_test)\n",
    "    timetaken = time.time() - start_time\n",
    "    kpredictiontime[k] = timetaken\n",
    "    \n",
    "    scoring_metrics(y_test, y_pred, f\"{k}\")\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "print(ktrainingtime)\n",
    "print(kpredictiontime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiRQu2ekMSlv"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#dictionary to store processing times\n",
    "algotrainingtime = {}\n",
    "algopredictiontime = {}\n",
    "\n",
    "clf = RandomForestClassifier(criterion=\"gini\",\n",
    "                             min_samples_split = 20,\n",
    "                             min_samples_leaf = 6,\n",
    "                             max_depth = 100,\n",
    "                             n_estimators=500,\n",
    "                             random_state=5) #can put any number here\n",
    "start_time =time.time()\n",
    "clf.fit(x_train, y_train)\n",
    "rftrainingtimetaken = time.time() - start_time\n",
    "algotrainingtime[\"Random Forest\"] = rftrainingtimetaken\n",
    "\n",
    "start_time =time.time()\n",
    "y_pred = clf.predict(x_test)\n",
    "rfpredictiontimetaken = time.time() - start_time\n",
    "algopredictiontime[\"Random Forest\"] = rfpredictiontimetaken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiRQu2ekMSlv"
   },
   "source": [
    "# Processing Time Comparision [RF & KNN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the algotime dictionaries with KNN's results\n",
    "algotrainingtime[\"KNN\"] = ktrainingtime[133]\n",
    "algopredictiontime[\"KNN\"] = kpredictiontime[133]\n",
    "\n",
    "#Plot the processing time graphs\n",
    "plt.title('Processing Time')\n",
    "plt.xlabel('ML Algorithm')\n",
    "\n",
    "trainingtime = algotrainingtime.values()\n",
    "predictiontime = algopredictiontime.values()\n",
    "\n",
    "x_axis = np.arange(len(algotrainingtime))\n",
    "width = 0.2\n",
    "#multi bar charts\n",
    "plt.bar(x_axis, trainingtime, color = 'b', width = 0.3, edgecolor = 'black',label='Training Time')\n",
    "plt.bar(x_axis + width, predictiontime, color = 'g',width = 0.3, edgecolor ='black',label='Prediction Time')\n",
    "\n",
    "plt.xticks(x_axis,algotrainingtime.keys())\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig('static/ProcessingTime_Comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiRQu2ekMSlv"
   },
   "source": [
    "# Random Forest's Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score,recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "## ==== CONFUSION MATRIX ====\n",
    "# Get and reshape confusion matrix data\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Build the plot\n",
    "plt.figure(figsize=(16,7))\n",
    "sb.set(font_scale=1.4)\n",
    "sb.heatmap(matrix, annot=True, annot_kws={'size':10},\n",
    "            cmap=plt.cm.Greens, linewidths=0.2)\n",
    "\n",
    "# Add labels to the plot\n",
    "class_names = ['-','nmap_scan', 'port_scan', 'smtp_enumeration', 'sql_enumeration', 'web_enumeration']\n",
    "tick_marks = np.arange(len(class_names))\n",
    "tick_marks2 = tick_marks + 0.5\n",
    "plt.xticks(tick_marks, class_names, rotation=25)\n",
    "plt.yticks(tick_marks2, class_names, rotation=0)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix for Random Forest Model')\n",
    "plt.show()\n",
    "\n",
    "RF_accuracy= accuracy_score(y_test, y_pred)\n",
    "algo_accuracy[\"Random Forest\"]= RF_accuracy\n",
    "\n",
    "RF_precision = precision_score(y_test, y_pred,average=\"weighted\")\n",
    "algo_precision[\"Random Forest\"] = RF_precision\n",
    "\n",
    "R1_f1_score = f1_score(y_test, y_pred,average=\"weighted\")\n",
    "algo_f1[\"Random Forest\"]=R1_f1_score\n",
    "\n",
    "R1_recall = recall_score(y_test, y_pred,average=\"weighted\")\n",
    "algo_recall[\"Random Forest\"]=R1_recall\n",
    "\n",
    "print(\"Classification Report \\n\" , classification_report(y_pred, y_test, labels=['-','nmap_scan', 'port_scan', 'smtp_enumeration', 'sql_enumeration', 'web_enumeration'], output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiRQu2ekMSlv"
   },
   "source": [
    "# KNN's Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdnmWet-MQDw",
    "outputId": "9751dff5-db89-424c-f556-49a843b772db"
   },
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "categories = ['-','nmap_scan', 'port_scan', 'smtp_enumeration', 'sql_enumeration', 'web_enumeration']\n",
    "ax.set_xticklabels(categories, rotation=45)\n",
    "                   \n",
    "sb.heatmap(confusion_matrix, annot=True, fmt='0', cmap='Blues', xticklabels=categories, yticklabels=categories)\n",
    "plt.title('KNN Confusion Matrix')\n",
    "plt.savefig('static/KNN_ConfusionMatrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiRQu2ekMSlv"
   },
   "source": [
    "# KNN's optimal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCbXBW0jMQDx"
   },
   "outputs": [],
   "source": [
    "plt.title('Optimal K')\n",
    "plt.xlabel('K value')\n",
    "\n",
    "k_value = list(accuracy_dict.keys())\n",
    "\n",
    "algo_accuracy[\"KNN\"]= accuracy_dict['133']\n",
    "plt.plot(k_value, list(accuracy_dict.values()), label = \"Accuracy\", linestyle=\"-\")\n",
    "plt.plot(k_value, list(precision_dict.values()), label = \"Precision\", linestyle=\"--\")\n",
    "plt.plot(k_value, list(f1_dict.values()), label = \"F1 Score\", linestyle=\"-.\")\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig('static/KNN_graph.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiRQu2ekMSlv"
   },
   "source": [
    "# Accuracy & Precision Comparison (RF & KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Accuracy and Precision Comparison')\n",
    "plt.xlabel('ML Algorithm')\n",
    "\n",
    "#Defining data to display\n",
    "algo_accuracy[\"KNN\"]= accuracy_dict['133']\n",
    "algo_precision['KNN']= precision_dict['133']\n",
    "\n",
    "accuracy = algo_accuracy.values()\n",
    "precision = algo_precision.values()\n",
    "x_axis = np.arange(len(algo_accuracy))\n",
    "width = 0.2\n",
    "\n",
    "#multi bar charts\n",
    "plt.bar(x_axis, accuracy, color = 'b', width = 0.3, edgecolor = 'black',label='KNN')\n",
    "plt.bar(x_axis + width, precision, color = 'g',width = 0.3, edgecolor ='black',label='Random Forest')\n",
    "\n",
    "plt.xticks(x_axis,['Accuracy', 'Precision'])\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('static/AnP_Comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiRQu2ekMSlv"
   },
   "source": [
    "# Accuracy Comparison alone -dk if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.xlabel('ML Algorithm')\n",
    "\n",
    "algo_accuracy[\"KNN\"]= accuracy_dict['133']\n",
    "accuracy = algo_accuracy.values()\n",
    "\n",
    "x_axis = algo_accuracy.keys()\n",
    "plt.bar(x_axis, accuracy,edgecolor = 'black', color=['b', 'g'])\n",
    "\n",
    "plt.savefig('static/Accuracy_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiRQu2ekMSlv"
   },
   "source": [
    "# Precision Comparison alone -dk if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Precision Score')\n",
    "plt.xlabel('ML Algorithm')\n",
    "\n",
    "algo_precision[\"KNN\"]= precision_dict['133']\n",
    "precisionscores = algo_precision.values()\n",
    "\n",
    "x_axis = algo_precision.keys()\n",
    "\n",
    "plt.bar(x_axis, precisionscores,edgecolor = 'black', label = \"F1 Scores\", color=['b', 'g'])\n",
    "\n",
    "plt.savefig('static/precision_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiRQu2ekMSlv"
   },
   "source": [
    "# F1_Score Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('F1 Score')\n",
    "plt.xlabel('ML Algorithm')\n",
    "\n",
    "algo_f1[\"KNN\"]= f1_dict['133']\n",
    "f1_scores = algo_f1.values()\n",
    "print(f1_scores)\n",
    "x_axis = algo_f1.keys()\n",
    "print(x_axis)\n",
    "plt.bar(x_axis, f1_scores,edgecolor = 'black', label = \"F1 Scores\", color=['b', 'g'])\n",
    "\n",
    "plt.savefig('static/F1_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiRQu2ekMSlv"
   },
   "source": [
    "# Recall Score Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Recall Score')\n",
    "plt.xlabel('ML Algorithm')\n",
    "\n",
    "algo_recall[\"KNN\"]= recall_dict['133']\n",
    "recall = algo_recall.values()\n",
    "print(recall)\n",
    "x_axis = algo_recall.keys()\n",
    "print(x_axis)\n",
    "plt.bar(x_axis, recall,edgecolor = 'black', label = \"recall\", color=['b', 'g'])\n",
    "\n",
    "plt.savefig('static/Recall_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLGFsrgfMQD7"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from flask import Flask, render_template, Response, url_for\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('help.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
