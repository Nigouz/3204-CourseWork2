{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZhD1QKdgcK9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import seaborn as sb\n",
    "import flask\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VY4YBaFhgmtT"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cu1B4nd9gmq8",
    "outputId": "42ea020f-2338-4d4c-9f1b-6916a434622f"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "# full_df = pd.read_csv(r\"C:\\Users\\Nicz\\Documents\\GitHub\\3204-CourseWork2\\shuffled-noIPV6.csv\")\n",
    "full_df = pd.read_csv(r\"shuffled-noIPV6.csv\")\n",
    "\n",
    "print(f\"[*] Shape of dataset: {full_df.shape}\")\n",
    "from sklearn.utils import shuffle\n",
    "full_df.drop(full_df.columns[0], axis=1, inplace=True)\n",
    "full_df = shuffle(full_df)\n",
    "\n",
    "print(full_df.tail())\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOgcXWdIzNSq"
   },
   "outputs": [],
   "source": [
    "full_df[\"src_port\"]=  full_df[\"src_port\"].replace(regex=\",\", value= \"\")\n",
    "full_df[\"src_port\"]=  full_df[\"src_port\"].replace(regex=\",\", value= \"\")\n",
    "full_df[\"dst_port\"]=  full_df[\"dst_port\"].replace(regex=\",\", value= \"\")\n",
    "full_df[\"dst_port\"]=  full_df[\"dst_port\"].replace(regex=\" \", value= \"\")\n",
    "full_df[\"src_port\"] = full_df[\"src_port\"].replace(regex=\" \", value=\"\")\n",
    "full_df[\"src_port\"] = full_df[\"src_port\"].replace(regex=\"dns\", value=\"53\")\n",
    "full_df[\"src_port\"] = full_df[\"src_port\"].replace(regex=\"tls\", value=\"0\")\n",
    "full_df[\"dst_port\"] = full_df[\"dst_port\"].replace(regex=\"dns\", value=\"53\")\n",
    "full_df[\"dst_ip\"] = full_df[\"dst_ip\"].replace(regex=\"\\S*:+\\S+\", value=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrekodUTmBym"
   },
   "outputs": [],
   "source": [
    "scores = {}\n",
    "scores_list= []\n",
    "\n",
    "k_value=[]\n",
    "model_scores={}\n",
    "accuracy_dict={}\n",
    "precision_dict={}\n",
    "cm_dict={}\n",
    "recall_dict={}\n",
    "f1_dict={}\n",
    "algo_accuracy={}\n",
    "algo_precision={}\n",
    "algo_recall={}\n",
    "algo_f1={}\n",
    "\n",
    "def scoring_metrics(y_test, y_pred, model):\n",
    "    print(f\"y_test size:{y_test.size} y_pred size:{y_pred.size}\")\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    recall = metrics.recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    \n",
    "    scores[model] = accuracy\n",
    "    scores_list.append(accuracy)\n",
    "#     confusion_matrix = metrics.confusion_matrix(y_true=[True, True], y_pred=[True, True], labels=[True, False])\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred, labels=['ldap_enumeration','nmap_scan', 'port_scan', 'smtp_enumeration', 'sql_enumeration', 'web_enumeration'])\n",
    "    \n",
    "    k_value.append(model)\n",
    "    accuracy_dict[model]= accuracy\n",
    "    precision_dict[model]= precision\n",
    "    recall_dict[model]= recall\n",
    "    f1_dict[model]= f1_score\n",
    "    \n",
    "    algo_precision[\"KNN\"]=precision\n",
    "    algo_recall[\"KNN\"]=recall\n",
    "    algo_f1[\"KNN\"]=f1_score\n",
    "        \n",
    "    print(f\"Confusion Matrix: {cm}\")\n",
    "    \n",
    "    print(f\"\\n[*] Model: {model}\")\n",
    "    print(\"[*]Precision: {:.3f}%\".format(precision))\n",
    "    print(\"[*] Recall: {:.3f}%\".format(recall))\n",
    "\n",
    "    print(\"[*] Accuracy: {:.3f}%\".format(accuracy))\n",
    "    print(\"[*] F1_score: {:.3f}%\".format(f1_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiRQu2ekMSlv"
   },
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7LbyI4qMQDu"
   },
   "outputs": [],
   "source": [
    "df = full_df.head(25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qd8IISrRwcsl",
    "outputId": "8006e4d9-070e-4cec-9d2d-65b0caeb6d4f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Doing\")\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "df.replace(to_replace=[\"None\"], value=np.nan, inplace=True)\n",
    "clean_df = df.fillna(str(0)) #uhm i dont knowhow to explain this, but please lmk another alternative to fix this cause :\") try to run it without str and you will know what i mean\"\n",
    "clean_x = clean_df.iloc[:, :13].values\n",
    "clean_y = clean_df[\"category\"].values\n",
    "features = df.columns.values[:-1]\n",
    "\n",
    "for label in clean_df.columns:\n",
    "    for index, rows in clean_df.iterrows():\n",
    "        new_ip = \"\"\n",
    "        ip = str(rows[label])\n",
    "        if re.search(\"\\d+\\.\\d+\\.\\d+\\.\\d+\", ip):\n",
    "            octets = ip.split(\".\")\n",
    "            for octet in octets:\n",
    "                octet = octet.rjust(3,\"0\")\n",
    "                new_ip += octet\n",
    "            clean_df[label][index] = new_ip\n",
    "\n",
    "clean_df[\"http_response_code\"] = clean_df[\"http_response_code\"].replace('HTTP/1.1\"', value=\"0\")\n",
    "clean_df[\"src_ip\"] = clean_df[\"src_ip\"].replace('::1', value=\"0\")\n",
    "clean_df[\"dst_ip\"] = clean_df[\"dst_ip\"].replace('::1', value=\"0\")\n",
    "# clean_df[\"http_response_code\"] = clean_df[\"http_response_code\"].replace('HTTP/1.1\"', value=\"0\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HO2a6ef4Cxl3"
   },
   "outputs": [],
   "source": [
    "clean_x = clean_df.iloc[:, :13] #honestly don't know if this is correct lmaoooo\n",
    "#this is to iterate through the columns and convert the strings to float (passthrough means for those columns we didnt specify, leave it as it is)\n",
    "column_trans = make_column_transformer((OneHotEncoder(sparse=False), ['Protocol', 'http_request_method', 'http_request_referrer', 'url_path', 'user_agent_original', 'sql_method', 'sql_query']),remainder='passthrough')\n",
    "test = column_trans.fit_transform(clean_x) #this is technically our cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZM3UFeBwmLhY"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(test, clean_y, test_size=0.3)\n",
    "# x_variable_train , x_variable_test , y_variable_train , y_variable_test  = train_test_split(test, clean_y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2zTbwdWmpJ6"
   },
   "outputs": [],
   "source": [
    "# k = 5\n",
    "# knn = KNeighborsClassifier(n_neighbors=k)\n",
    "# knn.fit(x_train, y_train)\n",
    "# # print(f\"Saving trained model to test-knn{k}.sav\")\n",
    "# # pickle.dump(knn, open(f\"test-knn{k}.sav\", \"wb\"))\n",
    "# # print(\"Model Saved\")\n",
    "\n",
    "# # print(\"Loading knn from pickle\")\n",
    "# # knn = pickle.load(open(\"knn5.sav\", \"rb\"))\n",
    "# # print(\"KNN Successfully loaded\")\n",
    "\n",
    "# y_pred = knn.predict(x_test)\n",
    "# scoring_metrics(y_test, y_pred, f\"knn {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0mYlTn8MQDw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = {}\n",
    "scores_list= []\n",
    "\n",
    "range_k = range(100, 160, 11)\n",
    "knn = \"\"\n",
    "skip = False\n",
    "\n",
    "for k in range_k:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train, y_train)\n",
    "#     pickle.dump(knn, open(f\"knn{k}.sav\", \"wb\"))\n",
    "\n",
    "    y_pred = knn.predict(x_test)\n",
    "    scoring_metrics(y_test, y_pred, f\"knn {k}\")\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiRQu2ekMSlv"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(criterion=\"gini\",\n",
    "                             min_samples_split = 20,\n",
    "                             min_samples_leaf = 6,\n",
    "                             max_depth = 100,\n",
    "                             n_estimators=500,\n",
    "                             random_state=5) #can put any number here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score,recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "## ==== CONFUSION MATRIX ====\n",
    "# Get and reshape confusion matrix data\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Build the plot\n",
    "plt.figure(figsize=(16,7))\n",
    "sb.set(font_scale=1.4)\n",
    "sb.heatmap(matrix, annot=True, annot_kws={'size':10},\n",
    "            cmap=plt.cm.Greens, linewidths=0.2)\n",
    "\n",
    "# Add labels to the plot\n",
    "class_names = ['ldap_enumeration','nmap_scan', 'port_scan', 'smtp_enumeration', 'sql_enumeration', 'web_enumeration']\n",
    "tick_marks = np.arange(len(class_names))\n",
    "tick_marks2 = tick_marks + 0.5\n",
    "plt.xticks(tick_marks, class_names, rotation=25)\n",
    "plt.yticks(tick_marks2, class_names, rotation=0)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix for Random Forest Model')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "RF_accuracy= accuracy_score(y_test, y_pred)\n",
    "algo_accuracy[\"Random Forest\"]= RF_accuracy\n",
    "\n",
    "precision = precision_score(y_test, y_pred,average=\"weighted\")\n",
    "algo_precision[\"RF\"] = precision\n",
    "\n",
    "f1_score = f1_score(y_test, y_pred,average=\"weighted\")\n",
    "algo_f1[\"RF\"]=f1_score\n",
    "\n",
    "recall = recall_score(y_test, y_pred,average=\"weighted\")\n",
    "algo_recall[\"RF\"]=recall\n",
    "\n",
    "\n",
    "print(\"Classification Report \\n\" , classification_report(y_pred, y_test, labels=['ldap_enumeration','nmap_scan', 'port_scan', 'smtp_enumeration', 'sql_enumeration', 'web_enumeration'], output_dict=True))\n",
    "#print(\"Accuracy Score:\\n\",(accuracy_score(y_test, y_pred)))\n",
    "#print(\"Precision Score:\\n\",(precision_score(y_test, y_pred,average=\"weighted\")))\n",
    "#print(\"Precision Score:\\n\",(f1_score(y_test, y_pred,average=\"weighted\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdnmWet-MQDw",
    "outputId": "9751dff5-db89-424c-f556-49a843b772db"
   },
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "categories = ['ldap_enumeration','nmap_scan', 'port_scan', 'smtp_enumeration', 'sql_enumeration', 'web_enumeration']\n",
    "ax.set_xticklabels(categories, rotation=45)\n",
    "                   \n",
    "sb.heatmap(confusion_matrix, annot=True, fmt='0', cmap='Blues', xticklabels=categories, yticklabels=categories)\n",
    "plt.title('KNN Confusion Matrix')\n",
    "plt.savefig('KNN_ConfusionMatrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCbXBW0jMQDx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title('K value variation')\n",
    "plt.xlabel('K value')\n",
    "plt.ylabel('Model Accuracy')\n",
    "\n",
    "k_value = list(accuracy_dict.keys())\n",
    "algo_accuracy[\"KNN\"]= accuracy_dict['knn 133']\n",
    "plt.plot(k_value, list(accuracy_dict.values()), label = \"Accuracy\", linestyle=\"-\")\n",
    "plt.plot(k_value, list(precision_dict.values()), label = \"Precision\", linestyle=\"--\")\n",
    "plt.plot(k_value, list(f1_dict.values()), label = \"F1 Score\", linestyle=\"-.\")\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig('KNN_graph.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Accuracy Comparison')\n",
    "plt.xlabel('ML Algorithm')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "print(algo_recall)\n",
    "plt.bar(algo_accuracy.keys(), algo_accuracy.values(), color ='maroon',width = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tLGFsrgfMQD7"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from flask import Flask, render_template, Response\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "#     return \"Welcome\"\n",
    "    knn = \"\"\"\n",
    "    # <img src=\"knn_variaion.png\" alt=\"Plot\">\n",
    "    # <img src=\"algo_accuracy.png\" alt=\"Plot\">   \n",
    "    \"\"\"\n",
    "    return (knn)\n",
    "@app.route('/knn_variaion.png')\n",
    "def plot_knn_variaion():\n",
    "    # Generate Graph Function\n",
    "    def knn_variaion():\n",
    "        figure, axis = plt.subplots(1, 2)\n",
    "        models = [100, 111, 122, 133, 144, 155]\n",
    "        y1 = [99, 100, 122, 133, 144, 155]\n",
    "        y2 = [100, 100, 122, 133, 144, 155]\n",
    "        y3 = [90, 90, 122, 133, 144, 155]\n",
    "\n",
    "        # For F1_score\n",
    "        axis[0].plot(models, y1, label=\"F1\", linestyle=\"-\")\n",
    "        axis[0].legend()\n",
    "        axis[0].set_title(\"F1_score\")\n",
    "\n",
    "        # For Accuracy/Precision\n",
    "        axis[1].plot(models, y2, label=\"accuracy\", linestyle=\"-\")\n",
    "        axis[1].plot(models, y3, label=\"precision\", linestyle=\"--\")\n",
    "        axis[1].legend()\n",
    "        axis[1].set_title(\"Accuracy/Precision\")\n",
    "        return figure\n",
    "    # Generate graph and return bytes\n",
    "    knn_variaion = knn_variaion()\n",
    "    output = io.BytesIO()\n",
    "    FigureCanvas(knn_variaion).print_png(output)\n",
    "    return Response(output.getvalue(), mimetype='image/png')\n",
    "\n",
    "\n",
    "@app.route('/algo_accuracy.png')\n",
    "def plot_algo_accuracy():\n",
    "    # Generate Graph Function\n",
    "    def algo_accuracy():\n",
    "        fig = Figure()\n",
    "        axis = fig.add_subplot(1, 1, 1)\n",
    "        algo_accuracy = {'KNN': 0.9849333333333333, 'RF': 0.7262666666666666}\n",
    "        axis.bar(algo_accuracy.keys(), algo_accuracy.values(), color='maroon', width=0.2)\n",
    "        return fig\n",
    "    \n",
    "    # Generate graph and return bytes\n",
    "    algo_accuracy = algo_accuracy()\n",
    "    o = io.BytesIO()\n",
    "    FigureCanvas(algo_accuracy).print_png(o)\n",
    "    return Response(o.getvalue(), mimetype='image/png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [21/Nov/2022 00:06:20] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [21/Nov/2022 00:06:20] \"\u001b[37mGET /knn_variaion.png HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [21/Nov/2022 00:06:20] \"\u001b[37mGET /algo_accuracy.png HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [21/Nov/2022 00:06:33] \"\u001b[37mGET /knn_variaion.png HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
